\documentclass[a4paper,11pt]{article}

%all english is included in french
\usepackage[french,english]{babel}
\usepackage[top=1cm, bottom=1.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{enumerate}

%the following is copy-paste
%%%% Drawing packages %%%%
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{graphicx}
\usepackage{adjustbox} % Used to constrain images to a maximum size 

%%%% Maths packages %%%%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{setspace}
\usepackage{mathrsfs}
\usepackage{MnSymbol}
\usepackage{latexsym}
\usepackage[makeroom]{cancel}

% package for drawing
\usepackage{palatino}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}

% Defines a `datastore' shape for use in DFDs.  This inherits from a
% rectangle and only draws two horizontal lines.
\makeatletter
\pgfdeclareshape{datastore}{
  \inheritsavedanchors[from=rectangle]
  \inheritanchorborder[from=rectangle]
  \inheritanchor[from=rectangle]{center}
  \inheritanchor[from=rectangle]{base}
  \inheritanchor[from=rectangle]{north}
  \inheritanchor[from=rectangle]{north east}
  \inheritanchor[from=rectangle]{east}
  \inheritanchor[from=rectangle]{south east}
  \inheritanchor[from=rectangle]{south}
  \inheritanchor[from=rectangle]{south west}
  \inheritanchor[from=rectangle]{west}
  \inheritanchor[from=rectangle]{north west}
  \backgroundpath{
    %  store lower right in xa/ya and upper right in xb/yb
    \southwest \pgf@xa=\pgf@x \pgf@ya=\pgf@y
    \northeast \pgf@xb=\pgf@x \pgf@yb=\pgf@y
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@ya}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@ya}}
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@yb}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@yb}}
 }
}
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

%language package
\usepackage[OT1]{fontenc}
%my own needed package



%%%% 	Commandes perso	 	%%%%
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathscr{D}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\F}{\mathscr{F}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\K}{\mathbb{K}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\m}{\mathfrak{m}}
\newcommand{\M}{\mathscr{M}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathscr{P}}
\newcommand{\prob}{\mathbf{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\X}{\mathfrak{X}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\esp}{\mathbf{E}}
\newcommand{\eps}{\varepsilon}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\ls}{\leqslant}
\newcommand{\gs}{\geqslant}

\newcommand{\ds}{\displaystyle}

\newcommand{\myindic}[1]{\mathds{1}_{#1}}
\newcommand{\myesp}[1]{\E\lp #1\rp}
\newcommand{\myprob}[1]{\prob\lp #1\rp}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\newcommand{\mylim}[2]{\underset{#1\rightarrow #2}{\longrightarrow}}
%\newcommand{\mylim}[3]{\underset{#1\rightarrow #2}{\overset{}{\longrightarrow}}}

%\renewcommand{\@seccntformat}[1]{%
%  \ifcsname prefix@#1\endcsname
%    \csname prefix@#1\endcsname
%  \else
%    \csname the#1\endcsname\quad
%  \fi}
% define \prefix@section
%\newcommand\prefix@section{Section \thesection: }
\renewcommand{\thesection}{\Roman{section}} 

%%%%%%%%%%%%%%%%%%%






%%%% Some useful maths environments %%%%
\theoremstyle{definition}

\newtheorem{exo}{Exercise}[section]
\newtheorem{quest}{Question}[section]
%\newtheorem{quests}[quest]{Questions}%[section]
\newtheorem{quests}{Questions}
\newtheorem{theo}{Theorem}%[subsection]
\newtheorem{de}[theo]{Definition}%[section]
\newtheorem{theode}[theo]{Theorem-Definition}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{propde}[theo]{Proposition-Definition}
\newtheorem{lem}[theo]{Lemma}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{ex}[theo]{Example}%[section]
\newtheorem{exs}[theo]{Examples}
\newtheorem{rem}[theo]{Remark}%[section]
\newtheorem{rems}[theo]{Remarks}%[section]

%synthese of correction
% \newenvironment{nam}[args]{begdef}{enddef}
%\renewenvironment{nam}[args]{begdef}{enddef}
\newenvironment{correction}{\footnotesize%
\noindent\rule{0.25\linewidth}{0.1pt}\\
\noindent\nobreak\@}{}


\title{Machine Learning Assignment 1}
\author{\bsc{Gansheng TAN }}
\date{\today}


\begin{document}

\maketitle

\section{The Learning Problem}
\begin{quests}
(b)\\
Mainly there are three types of machine learning strategies. The supervised, unsupervised
and reinforcement strategies. The unsupervised strategy is used to data clustering, the supervised strategy is used for classification, while the reinforcement learning is used to game playing and robot controlling. One should know that learning is used when human expertise does not exist or humans are unable to explain their expertise or complex solutions change in time\\
\\
There exists some mathematical rules to determine a number is prime or not, so to speak, human expertise, so (a) can be solved by classical program. Newtons laws of motions show us the pattern exist, so (c) is as same as (a), the classical can give us an exact solution, while learning from data will lead us to an approximate solution.
Humans are less able to identify the pattern of fraud, however, machine learning systems can analyze vast amounts of historical data to identify patterns associated with fraud, and certainly we have lots of hostorical data, so (b) can be a classification problem in machine learning.\\
\\
As far as I am concerned, (d) can be done by machine learning, we can assume that if there are lots of car in one direction, we should increase the time of green light on this direction. So, it's more like a supervised problem. However, in the aspect of labeled data, we don't have much unless we do some experiment. Since the change of the green light time can cause a big problem, we shouldn't do it frequently.
\end{quests}

\newpage

\begin{quests}
\leavevmode\\
It's a supervised classification machine learning problem which is a problem where we are using data to predict which category something falls into. The information of one article will be stored in a row and the predictor is $\in \{0,1\}$ \\
\begin{tikzpicture}[
  font=\sffamily,
  every matrix/.style={ampersand replacement=\&,column sep=5cm,row sep=2cm},
  source/.style={draw,thick,rounded corners,fill=yellow!20,inner sep=.3cm},
  process/.style={draw,thick,circle,fill=blue!20},
  sink/.style={source,fill=green!20},
  datastore/.style={draw,very thick,shape=datastore,inner sep=.3cm},
  dots/.style={gray,scale=2},
  to/.style={->,>=stealth',shorten >=1pt,semithick,font=\sffamily\footnotesize},
  every node/.style={align=center}]
  
  %define the arrow
  \tikzstyle{arrow} = [->,>=stealth,semithick]

  % Position the nodes using a matrix layout
  \matrix{
    \node[source] (hisparcbox) {start};
      \& \node[process] (input) {data input}; \& \\

    \node[process] (label) {label};
      \& \node[process] (ML) {ML algo};
      \& \node[sink] (predictor) {predictor}; \\
  };

  % Draw the arrows between the nodes and label them.
  \draw[to] (hisparcbox) -- node[midway,above] {the number of times of the  } (input)
      node[midway,below] {key words as a row} (input);
  \draw[to] (label) -- node[midway,above] {define whether this article is }
      node[midway,below] {a science policy or not} (ML);
  \draw[arrow] (input) -- (ML);
  \draw[to] (ML) -- node[midway,above] {Run the}
      node[midway,below] {algorithm} (predictor);
\end{tikzpicture}

\end{quests}

\newpage

\begin{quests}
\leavevmode\\
\begin{enumerate}[(a)]
\item No, it's not a ML problem because we can know the exact result by multiplying the volume of gas from the manual and the cost of gas.
\item Yes, it's a supervised classification ML problem.The label Y, data matrix X, predictor vector $\theta$ is presented below\\
\[X=
\begin{bmatrix}
    		& feature \quad from \quad building \quad  plan  & & parameter \quad from \quad engineer's \quad note   \\
    house 1 &       		.			   & &             .\\
    house 2 &				.			   & &             .\\
    house 3 &				.			   & &		      .\\
    \vdots  &						\vdots & &		\vdots
\end{bmatrix}
\]
\[
Y=
\begin{bmatrix}
    heating \quad load \quad of \quad  house 1     \\
    heating \quad load \quad of \quad  house 2		\\ 
    heating \quad load \quad of \quad  house 3      \\ 
    \vdots
\end{bmatrix}
\qquad \qquad \theta=
\begin{bmatrix}
    \theta_1  \\
    \theta_2	\\
    \theta_3  \\ 
    \vdots
\end{bmatrix}
\]
$\theta$ is used in $y^\star=\theta^T x^\star$ while x is a unknow house, ${x^\star}_i$ is its feature both from building plan and engineer's note
\end{enumerate}
\end{quests}

\newpage

\section{Dimention Reduction}
\begin{quests}
\leavevmode\\

\begin{enumerate}[(a)]

\item They are symmetric and real\\
Prove:
\begin{enumerate}[*]
\item symmetric: $(MM^{T})^{T} = (M^{T})^{T}M^{T} = MM^{T}$, same reason for $M^{T}M$
\item square: one has the difinition of matrix product, hence the dimension of $(MM^{T})$ and $(M^{T}M)$ are respectively ${m \times m}$ and ${n \times n}$. 
\item{
real: Not necessary if M is a complex matrix, for example 
\[\begin{bmatrix}
    $1+i$   & i  \\
    1       & 2 
\end{bmatrix}
\cdot
\begin{bmatrix}
    $1+i$   & 1  \\
    i       & 2 
\end{bmatrix}
=
\begin{bmatrix}
    $2i-1$   &$1-i$  \\
    1+3i     & 5 
\end{bmatrix}
\]
but it is true when $M$ is real. That's what we will admit for the following questions
}
\end{enumerate}
\item We use SVD for matrix $M$
$$M_{m \times n}=U_{m \times r}S_{r \times r}V_{n \times r}^T$$
because$UU^T=I$, we have
$$M^TM=(USV^T)^TUSV^T=VS^2V^T$$
we do the same to $MM^T$,We obtain
$$MM^T=USV^T(USV^T)^T=US^2U^T$$
Since $M^{T}M$ is a real symmetric matrix,, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix $M^{T}M$ and $MM^{T}$  can be decomposed as\\
$$\mathbf {M^{T}M} =\mathbf {Q} \mathbf {\Lambda } \mathbf {Q} ^{T}$$
$$\mathbf {MM^{T}} =\mathbf {Q}^\prime \mathbf {\Lambda }^\prime (\mathbf {Q}^\prime) ^{T}$$
where $\mathbf{Q}$ is an orthogonal matrix whose columns are the eigenvectors of $M$, and $\mathbf {\Lambda }$ is a diagonal matrix whose entries are the eigenvalues of  $\mathbf{Q}$.\\
\leavevmode\\
Then we notice that $\mathbf {\Lambda}=S^2=\mathbf {\Lambda }^\prime$\\
\leavevmode\\
The eigenvectors are not necessary the same because the dimension of two eigenvectors one of which is of $M^{T}M$ and the other is of $MM^{T}$ have different dimension. 
%given $\lambda$ a eigenvalue of $M^{T}M$, let $X_{n \times 1}$ a vector, then we have
%$$ M^{T}M X_{n \times 1} =\lambda X_{n \times 1} $$
%we will verify $\lambda$ is also a eigenvalue of $MM^{T}$, we have\\
%$$MM^{T} (X_{n \times 1}) = (M^{T}M X_{n \times 1})^T=(\lambda X_{n \times 1})^T
%=\lambda (X_{n \times 1})^T=$$
\item These expressions are written in answer (b)
\item we can see in (b), the singular of a matrix $ M_{m \times n}$ are the square roots of the eigenvalue of $MM^{T}$
\end{enumerate}
\begin{rem}
\leavevmode\\
If  M is a square invertible matrix, then we have a easier approach, yje cjaracteristic polynomial of $MM^{T}$: $p_{MM^{T}}(t)=\det \left(MM^{T}-tI\right)=\det \left(M \right)\det \left(MM^{T}-tI\right) \det \left(M^{-1} \right) = p_{M^{T}M}(t)$\\
Since they have same characteristic polynomial, they have same spectrum. In addition, we can extend this result to square mareix since invertible matrices are dense in square matrices, and the characteristic polynomial depends continuously on matrix.
\end{rem}
\end{quests}

\newpage
\begin{quests}
\leavevmode\\
\begin{enumerate}[(a)]
\item the first principal axis is $(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})^T$ by calulating the eigenvector of $MM^T$, where $M$ is
$$
\left\{
\begin{matrix}
-1  &  -1\\
0   &  0\\
1	&  1
\end{matrix}
\right\}
$$
\item The mathematical way of projection is to the scalar product of sample vector ${x_1}^T=(-1,-1)^T$,${x_2}^T=(0,0)^T$,${x_3}^T=(1,1)^T$ and the first pricipal $z=(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})^T$.
so ${x_1}^\prime={x_1}^Tz=-\sqrt{2}$, ${x_2}^\prime=0$, ${x_3}^\prime=\sqrt{2}$.

\item $$
Var=\frac{1}{3}\sum_{i=1}^3(x_i^\prime-0)=\frac{3}{4}
$$
\end{enumerate}
\end{quests}

\newpage

\section{Model Evaluation and Selection}
\begin{quests}
\leavevmode\\
Relating the concept of confusion matrix that we see in course, we can denote \\
$$\begin{matrix}
(labeled edible, edible) &is& True Negative & \textbf{TN}\\
(labeled edible, poisonous) &is& False Positive & \textbf{FP}\\
(labeled poisonous, poisonous) &is& True Positive & \textbf{TP}\\
(labeled poisonous, edible) &is& False Negative & \textbf{FN}
\end{matrix}$$
\begin{enumerate}[(a)]
\item Accuracy(Algo1)=$\dfrac{TP+TN}{TP+FN+FP+TN}=\dfrac{97+100}{200}=0.985$\\
Accuracy(Algo2)=$\dfrac{TP+TN}{TP+FN+FP+TN}=\dfrac{96+100}{200}=0.98$
\item Algorithme 1, because Algorithme 1 classifies all the lebaled poisonous mushrooms as poisonous, contrarily Algorithme 2 has type \Rmnum{2} error which is the missing error. So in our case, we want to assure that every poisonous mushroom will be classified as poisonous, so we want to minimize the false negative rate(\textbf{FN}). That's why we choose Algo 2.
\end{enumerate}
\end{quests}

\newpage
\begin{quests}
\leavevmode
\\
False Positive Rate. We want to minimise unditected(predicted condition is negative, which is false in reality) fraud (True condition is positive),that is to say FP, given a total number of transaction, we want to minimize FPR.
\end{quests}
\newpage

\section{Linear Regression, Logistic Regression and Feature Selection}
\begin{quests}
\leavevmode
\\
\begin{enumerate}[(a)]
\item Using the notation in class, we have $x^{(i}$ standing for $i^{th}$sample, and $Y_{n \times 1}$ the label. After tedious computation in paper(it's not so readable, so...), we have 
$$\nabla_{\theta}J_{RR}(\theta)=X^T(X\theta-Y)+\lambda I\theta$$
So the closed form solution is when $\nabla_{\theta}J_{RR}(\theta)=0$
$$\hat{\theta} = (X^{T}X + \lambda I)^{-1}X^{T}Y$$

\item L2 regularization term prefers parameters close to zero, so it is more robust to overfitting.
\end{enumerate}
\end{quests}
\newpage

\begin{quests}
\begin{enumerate}[1]
\leavevmode
\\
\item the area under curve is 0.9775851267688679, the required running time is 5.332571918898696. The implementation in Python is shown below.\\

\item We can see from the curve: feature selection improve the accuracy,but the accuracy is tending to saturation, we observe that more feature result to less running time.
\begin{center}
\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_2_0.png}
\end{center}
\end{enumerate}
\end{quests}




\end{document}
